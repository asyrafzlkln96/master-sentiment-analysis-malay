{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Fine-tune BERT BASE model",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Xa6LigHuLUE",
        "outputId": "acdbe649-1f69-4766-b300-7463acb46b89"
      },
      "source": [
        "# Install tf libraries\n",
        "!pip3 install bert-tensorflow==1.0.1 tensorflow==1.15"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting bert-tensorflow==1.0.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a6/66/7eb4e8b6ea35b7cc54c322c816f976167a43019750279a8473d355800a93/bert_tensorflow-1.0.1-py2.py3-none-any.whl (67kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 10.0MB/s \n",
            "\u001b[?25hCollecting tensorflow==1.15\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/92/2b/e3af15221da9ff323521565fa3324b0d7c7c5b1d7a8ca66984c8d59cb0ce/tensorflow-1.15.0-cp37-cp37m-manylinux2010_x86_64.whl (412.3MB)\n",
            "\u001b[K     |████████████████████████████████| 412.3MB 38kB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from bert-tensorflow==1.0.1) (1.15.0)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (0.8.1)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (1.1.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (1.12.1)\n",
            "Collecting gast==0.2.2\n",
            "  Downloading https://files.pythonhosted.org/packages/4e/35/11749bf99b2d4e3cceb4d55ca22590b0d7c2c62b9de38ac4a4a7f4687421/gast-0.2.2.tar.gz\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (0.36.2)\n",
            "Collecting tensorboard<1.16.0,>=1.15.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1e/e9/d3d747a97f7188f48aa5eda486907f3b345cd409f0a0850468ba867db246/tensorboard-1.15.0-py3-none-any.whl (3.8MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8MB 51.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (3.3.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (0.2.0)\n",
            "Collecting tensorflow-estimator==1.15.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/de/62/2ee9cd74c9fa2fa450877847ba560b260f5d0fb70ee0595203082dafcc9d/tensorflow_estimator-1.15.1-py2.py3-none-any.whl (503kB)\n",
            "\u001b[K     |████████████████████████████████| 512kB 50.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (0.10.0)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (1.19.5)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (3.12.4)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (1.32.0)\n",
            "Collecting keras-applications>=1.0.8\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/71/e3/19762fdfc62877ae9102edf6342d71b28fbfd9dea3d2f96a882ce099b03f/Keras_Applications-1.0.8-py3-none-any.whl (50kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 8.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (1.1.2)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (1.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (3.3.4)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (53.0.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras-applications>=1.0.8->tensorflow==1.15) (2.10.0)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (3.7.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (3.4.0)\n",
            "Building wheels for collected packages: gast\n",
            "  Building wheel for gast (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gast: filename=gast-0.2.2-cp37-none-any.whl size=7540 sha256=2746d216d1ed429e21789d29ed944fb61954ef267fd87e6ee52c8e456d569404\n",
            "  Stored in directory: /root/.cache/pip/wheels/5c/2e/7e/a1d4d4fcebe6c381f378ce7743a3ced3699feb89bcfbdadadd\n",
            "Successfully built gast\n",
            "\u001b[31mERROR: tensorflow-probability 0.12.1 has requirement gast>=0.3.2, but you'll have gast 0.2.2 which is incompatible.\u001b[0m\n",
            "Installing collected packages: bert-tensorflow, gast, tensorboard, tensorflow-estimator, keras-applications, tensorflow\n",
            "  Found existing installation: gast 0.3.3\n",
            "    Uninstalling gast-0.3.3:\n",
            "      Successfully uninstalled gast-0.3.3\n",
            "  Found existing installation: tensorboard 2.4.1\n",
            "    Uninstalling tensorboard-2.4.1:\n",
            "      Successfully uninstalled tensorboard-2.4.1\n",
            "  Found existing installation: tensorflow-estimator 2.4.0\n",
            "    Uninstalling tensorflow-estimator-2.4.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.4.0\n",
            "  Found existing installation: tensorflow 2.4.1\n",
            "    Uninstalling tensorflow-2.4.1:\n",
            "      Successfully uninstalled tensorflow-2.4.1\n",
            "Successfully installed bert-tensorflow-1.0.1 gast-0.2.2 keras-applications-1.0.8 tensorboard-1.15.0 tensorflow-1.15.0 tensorflow-estimator-1.15.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4GEORfzruW0a",
        "outputId": "fdf07d43-1c6f-446b-8a1b-5e6ee1f34375"
      },
      "source": [
        "# Download pre-trained model BERT-Base\n",
        "!wget https://f000.backblazeb2.com/file/malaya-model/bert-bahasa/bert-base-2020-10-08.tar.gz\n",
        "!wget https://raw.githubusercontent.com/huseinzol05/Malaya/master/pretrained-model/bert/BERT.wordpiece\n",
        "!wget https://raw.githubusercontent.com/huseinzol05/Malaya/master/pretrained-model/bert/config/BASE_config.json\n",
        "!tar -zxf bert-base-2020-10-08.tar.gz\n",
        "!ls"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-02-27 06:23:01--  https://f000.backblazeb2.com/file/malaya-model/bert-bahasa/bert-base-2020-10-08.tar.gz\n",
            "Resolving f000.backblazeb2.com (f000.backblazeb2.com)... 104.153.233.177\n",
            "Connecting to f000.backblazeb2.com (f000.backblazeb2.com)|104.153.233.177|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 \n",
            "Length: 1217112118 (1.1G) [application/octet-stream]\n",
            "Saving to: ‘bert-base-2020-10-08.tar.gz’\n",
            "\n",
            "bert-base-2020-10-0 100%[===================>]   1.13G  29.3MB/s    in 44s     \n",
            "\n",
            "2021-02-27 06:23:46 (26.5 MB/s) - ‘bert-base-2020-10-08.tar.gz’ saved [1217112118/1217112118]\n",
            "\n",
            "--2021-02-27 06:23:46--  https://raw.githubusercontent.com/huseinzol05/Malaya/master/pretrained-model/bert/BERT.wordpiece\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 224153 (219K) [text/plain]\n",
            "Saving to: ‘BERT.wordpiece’\n",
            "\n",
            "BERT.wordpiece      100%[===================>] 218.90K  --.-KB/s    in 0.005s  \n",
            "\n",
            "2021-02-27 06:23:46 (42.9 MB/s) - ‘BERT.wordpiece’ saved [224153/224153]\n",
            "\n",
            "--2021-02-27 06:23:46--  https://raw.githubusercontent.com/huseinzol05/Malaya/master/pretrained-model/bert/config/BASE_config.json\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 503 [text/plain]\n",
            "Saving to: ‘BASE_config.json’\n",
            "\n",
            "BASE_config.json    100%[===================>]     503  --.-KB/s    in 0s      \n",
            "\n",
            "2021-02-27 06:23:46 (37.1 MB/s) - ‘BASE_config.json’ saved [503/503]\n",
            "\n",
            "BASE_config.json\t     BERT.wordpiece  sentiment-data-v2.csv\n",
            "bert-base\t\t     drive\t     utils.py\n",
            "bert-base-2020-10-08.tar.gz  sample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ItSNRKP3uxY2",
        "outputId": "f6801d6f-5b2f-4b97-a3b4-c44519337451"
      },
      "source": [
        "!ls bert-base"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "model.ckpt-1000000.data-00000-of-00001\tmodel.ckpt-1000000.meta\n",
            "model.ckpt-1000000.index\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dm5ML61-vPfp",
        "outputId": "2c8a91a7-8f73-4612-bd0d-f681ef045a25"
      },
      "source": [
        "!pip3 install utils\n",
        "import sys\n",
        "\n",
        "sys.path.insert(0, '../')\n",
        "import utils"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting utils\n",
            "  Downloading https://files.pythonhosted.org/packages/55/e6/c2d2b2703e7debc8b501caae0e6f7ead148fd0faa3c8131292a599930029/utils-1.0.1-py2.py3-none-any.whl\n",
            "Installing collected packages: utils\n",
            "Successfully installed utils-1.0.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        },
        "id": "n53bPyt4vsvC",
        "outputId": "1f4651ee-f5bd-4d38-d547-58436cdaf809"
      },
      "source": [
        "# Load labelled sentiment data\n",
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv('sentiment-data-v2.csv')\n",
        "df.tail()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>3680</th>\n",
              "      <td>Positive</td>\n",
              "      <td>Jelas pembangkang buat tuduhan untuk mengeliru...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3681</th>\n",
              "      <td>Positive</td>\n",
              "      <td>demokrasi adalah kuasa rakyat di mana pegawai ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3682</th>\n",
              "      <td>Positive</td>\n",
              "      <td>Selain dapat menyelesaikan isu beg berat, peng...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3683</th>\n",
              "      <td>Positive</td>\n",
              "      <td>Hospital Langkawi buat masa ini hanya dapat me...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3684</th>\n",
              "      <td>Positive</td>\n",
              "      <td>Jika sebelum ini kita selesa bergerak dalam ‘g...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         label                                               text\n",
              "3680  Positive  Jelas pembangkang buat tuduhan untuk mengeliru...\n",
              "3681  Positive  demokrasi adalah kuasa rakyat di mana pegawai ...\n",
              "3682  Positive  Selain dapat menyelesaikan isu beg berat, peng...\n",
              "3683  Positive  Hospital Langkawi buat masa ini hanya dapat me...\n",
              "3684  Positive  Jika sebelum ini kita selesa bergerak dalam ‘g..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rMxdaQ15wUa2",
        "outputId": "a3130ad4-fce5-4625-c60d-20f8beed814d"
      },
      "source": [
        "# Convert df to list\n",
        "labels = df['label'].values.tolist()\n",
        "texts = df['text'].values.tolist()\n",
        "unique_labels = sorted(list(set(labels)))\n",
        "unique_labels"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Negative', 'Positive']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K4OKkF4fwlEM",
        "outputId": "8890cc9d-c677-4156-af1f-41d8bcdb2a18"
      },
      "source": [
        "import tensorflow as tf\n",
        "import bert\n",
        "from bert import run_classifier\n",
        "from bert import optimization\n",
        "from bert import tokenization\n",
        "from bert import modeling"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/bert/optimization.py:87: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Re0PIiXy2acY",
        "outputId": "24f0d181-c491-4c2d-a66b-f56bd78c940c"
      },
      "source": [
        "tokenizer = tokenization.FullTokenizer(vocab_file = 'BERT.wordpiece', do_lower_case = False)\n",
        "tokens = tokenizer.tokenize('Asyraf hensem sangat')\n",
        "tokens"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Asyraf', 'he', '##n', '##sem', 'sangat']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zYhQ_YBt6XjN",
        "outputId": "ecd9d430-debc-4bed-bd85-71a659360043"
      },
      "source": [
        "tokenizer.convert_tokens_to_ids(tokens)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[22589, 3788, 1015, 29643, 2521]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cftAaGlK8Qja"
      },
      "source": [
        "def token_to_ids(text, maxlen = 512):\n",
        "    tokens_a = tokenizer.tokenize(text)\n",
        "    if len(tokens_a) > maxlen - 2:\n",
        "        tokens_a = tokens_a[:(maxlen - 2)]\n",
        "    tokens = ['[CLS]'] + tokens_a + ['[SEP]']\n",
        "    segment_id = [0] * len(tokens)\n",
        "    input_mask = [1] * len(tokens)\n",
        "    input_id = tokenizer.convert_tokens_to_ids(tokens)\n",
        "    return {'tokens': tokens, 'input_id': input_id,\n",
        "    'input_mask': input_mask, 'segment_id': segment_id}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qBVRYRzD8aS2",
        "outputId": "3b493266-609d-4ff2-b23a-99d5009ac111"
      },
      "source": [
        "# Tokenize text \n",
        "token_to_ids(texts[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_id': [2,\n",
              "  4015,\n",
              "  17,\n",
              "  2009,\n",
              "  2088,\n",
              "  1822,\n",
              "  5714,\n",
              "  6332,\n",
              "  1766,\n",
              "  3062,\n",
              "  3558,\n",
              "  16,\n",
              "  20153,\n",
              "  1828,\n",
              "  3718,\n",
              "  2766,\n",
              "  20018,\n",
              "  18,\n",
              "  3],\n",
              " 'input_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
              " 'segment_id': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
              " 'tokens': ['[CLS]',\n",
              "  'Lebih',\n",
              "  '-',\n",
              "  'lebih',\n",
              "  'lagi',\n",
              "  'dengan',\n",
              "  'kemudahan',\n",
              "  'internet',\n",
              "  'dan',\n",
              "  'laman',\n",
              "  'sosial',\n",
              "  ',',\n",
              "  'taktik',\n",
              "  'ini',\n",
              "  'semakin',\n",
              "  'mudah',\n",
              "  'dikembangkan',\n",
              "  '.',\n",
              "  '[SEP]']}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QqS-9jYE9BXr"
      },
      "source": [
        "# Data pipeline\n",
        "def generate():\n",
        "    while True:\n",
        "        for i in range(len(texts)):\n",
        "            if len(texts[i]) > 5:\n",
        "                d = token_to_ids(texts[i])\n",
        "                d['label'] = [unique_labels.index(labels[i])]\n",
        "                d.pop('tokens', None)\n",
        "                yield d"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dMHdprZAAZK-",
        "outputId": "fd64a4df-34a3-4d50-ffd8-dd96075f2f69"
      },
      "source": [
        "g = generate()\n",
        "next(g)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_id': [2,\n",
              "  4015,\n",
              "  17,\n",
              "  2009,\n",
              "  2088,\n",
              "  1822,\n",
              "  5714,\n",
              "  6332,\n",
              "  1766,\n",
              "  3062,\n",
              "  3558,\n",
              "  16,\n",
              "  20153,\n",
              "  1828,\n",
              "  3718,\n",
              "  2766,\n",
              "  20018,\n",
              "  18,\n",
              "  3],\n",
              " 'input_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
              " 'label': [0],\n",
              " 'segment_id': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NGOR-hUpAlTM"
      },
      "source": [
        "def get_dataset(batch_size = 32, shuffle_size = 128):\n",
        "  # Shuffle_size 32 > 128 - 27/2/2021\n",
        "    def get():\n",
        "        dataset = tf.data.Dataset.from_generator(\n",
        "            generate,\n",
        "            {'input_id': tf.int32, 'input_mask': tf.int32, 'segment_id': tf.int32, 'label': tf.int32},\n",
        "            output_shapes = {\n",
        "                'input_id': tf.TensorShape([None]),\n",
        "                'input_mask': tf.TensorShape([None]),\n",
        "                'segment_id': tf.TensorShape([None]),\n",
        "                'label': tf.TensorShape([None])\n",
        "            },\n",
        "        )\n",
        "        dataset = dataset.shuffle(shuffle_size)\n",
        "        dataset = dataset.padded_batch(\n",
        "            batch_size,\n",
        "            padded_shapes = {\n",
        "                'input_id': tf.TensorShape([None]),\n",
        "                'input_mask': tf.TensorShape([None]),\n",
        "                'segment_id': tf.TensorShape([None]),\n",
        "                'label': tf.TensorShape([None])\n",
        "            },\n",
        "            padding_values = {\n",
        "                'input_id': tf.constant(0, dtype = tf.int32),\n",
        "                'input_mask': tf.constant(0, dtype = tf.int32),\n",
        "                'segment_id': tf.constant(0, dtype = tf.int32),\n",
        "                'label': tf.constant(0, dtype = tf.int32),\n",
        "            },\n",
        "        )\n",
        "        return dataset\n",
        "    return get"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tXeQKxHPA0UF",
        "outputId": "7c804fad-bda3-4c6b-d212-9e27c1d34141"
      },
      "source": [
        "# Test data pipeline using tf.session\n",
        "tf.reset_default_graph()\n",
        "sess = tf.InteractiveSession()\n",
        "iterator = get_dataset()()\n",
        "iterator = iterator.make_one_shot_iterator().get_next()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/client/session.py:1750: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
            "  warnings.warn('An interactive session is already active. This can '\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HaOXn0FNA890",
        "outputId": "ab964702-6a21-4a66-9d36-7139fe11321b"
      },
      "source": [
        "iterator"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_id': <tf.Tensor 'IteratorGetNext:0' shape=(?, ?) dtype=int32>,\n",
              " 'input_mask': <tf.Tensor 'IteratorGetNext:1' shape=(?, ?) dtype=int32>,\n",
              " 'label': <tf.Tensor 'IteratorGetNext:2' shape=(?, ?) dtype=int32>,\n",
              " 'segment_id': <tf.Tensor 'IteratorGetNext:3' shape=(?, ?) dtype=int32>}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o3FwPeBEBDI1",
        "outputId": "1db3ad5e-f242-4a10-9d26-60e4fd58b6f9"
      },
      "source": [
        "sess.run(iterator)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_id': array([[    2,  7824,  5119, ...,     0,     0,     0],\n",
              "        [    2,  2616, 11334, ...,     0,     0,     0],\n",
              "        [    2,  2508, 15201, ...,     0,     0,     0],\n",
              "        ...,\n",
              "        [    2,  3130,  1874, ...,     0,     0,     0],\n",
              "        [    2,  6542,  1011, ...,     0,     0,     0],\n",
              "        [    2,  3130,  7140, ...,     0,     0,     0]], dtype=int32),\n",
              " 'input_mask': array([[1, 1, 1, ..., 0, 0, 0],\n",
              "        [1, 1, 1, ..., 0, 0, 0],\n",
              "        [1, 1, 1, ..., 0, 0, 0],\n",
              "        ...,\n",
              "        [1, 1, 1, ..., 0, 0, 0],\n",
              "        [1, 1, 1, ..., 0, 0, 0],\n",
              "        [1, 1, 1, ..., 0, 0, 0]], dtype=int32),\n",
              " 'label': array([[1],\n",
              "        [1],\n",
              "        [1],\n",
              "        [0],\n",
              "        [1],\n",
              "        [1],\n",
              "        [0],\n",
              "        [1],\n",
              "        [1],\n",
              "        [1],\n",
              "        [1],\n",
              "        [0],\n",
              "        [0],\n",
              "        [1],\n",
              "        [1],\n",
              "        [1],\n",
              "        [0],\n",
              "        [0],\n",
              "        [1],\n",
              "        [1],\n",
              "        [0],\n",
              "        [0],\n",
              "        [1],\n",
              "        [0],\n",
              "        [0],\n",
              "        [1],\n",
              "        [1],\n",
              "        [1],\n",
              "        [0],\n",
              "        [1],\n",
              "        [0],\n",
              "        [0]], dtype=int32),\n",
              " 'segment_id': array([[0, 0, 0, ..., 0, 0, 0],\n",
              "        [0, 0, 0, ..., 0, 0, 0],\n",
              "        [0, 0, 0, ..., 0, 0, 0],\n",
              "        ...,\n",
              "        [0, 0, 0, ..., 0, 0, 0],\n",
              "        [0, 0, 0, ..., 0, 0, 0],\n",
              "        [0, 0, 0, ..., 0, 0, 0]], dtype=int32)}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cUzhBzCABoPe",
        "outputId": "641e9e1d-2b81-4629-b81b-a80dc926abb1"
      },
      "source": [
        "# Define BERT model\n",
        "bert_config = modeling.BertConfig.from_json_file('BASE_config.json')\n",
        "bert_config.__dict__"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'attention_probs_dropout_prob': 0.1,\n",
              " 'directionality': 'bidi',\n",
              " 'hidden_act': 'gelu',\n",
              " 'hidden_dropout_prob': 0.1,\n",
              " 'hidden_size': 768,\n",
              " 'initializer_range': 0.02,\n",
              " 'intermediate_size': 3072,\n",
              " 'max_position_embeddings': 512,\n",
              " 'num_attention_heads': 12,\n",
              " 'num_hidden_layers': 12,\n",
              " 'pooler_fc_size': 768,\n",
              " 'pooler_num_attention_heads': 12,\n",
              " 'pooler_num_fc_layers': 3,\n",
              " 'pooler_size_per_head': 128,\n",
              " 'pooler_type': 'first_token_transform',\n",
              " 'type_vocab_size': 2,\n",
              " 'vocab_size': 32000}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gnvFeSEtB5O4"
      },
      "source": [
        "epoch = 30\n",
        "warmup_proportion = 0.1\n",
        "num_warmup_steps = int(epoch * warmup_proportion)\n",
        "learning_rate = 2e-5\n",
        "init_checkpoint = 'bert-base/model.ckpt-1000000'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_QGhC3xsB-Pv"
      },
      "source": [
        "# Model_fn accept 4 parameters, define training loss and accuracy\n",
        "import utils_lib\n",
        "def model_fn(features, labels, mode, params):\n",
        "    Y = tf.cast(features['label'][:, 0], tf.int32)\n",
        "\n",
        "    model = modeling.BertModel(\n",
        "        config = bert_config,\n",
        "        is_training = True,\n",
        "        input_ids = features['input_id'],\n",
        "        input_mask = features['input_mask'],\n",
        "        token_type_ids = features['segment_id'],\n",
        "        # use_one_hot_embeddings = False,\n",
        "        use_one_hot_embeddings = True,\n",
        "    )\n",
        "    output_layer = model.get_pooled_output()\n",
        "    logits = tf.layers.dense(output_layer, 2)\n",
        "    loss = tf.reduce_mean(\n",
        "        tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
        "            logits = logits, labels = Y\n",
        "        )\n",
        "    )\n",
        "\n",
        "    tf.identity(loss, 'train_loss')\n",
        "\n",
        "    accuracy = tf.metrics.accuracy(\n",
        "        labels = Y, predictions = tf.argmax(logits, axis = 1)\n",
        "    )\n",
        "    tf.identity(accuracy[1], name = 'train_accuracy')\n",
        "\n",
        "    variables = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES)\n",
        "\n",
        "    assignment_map, initialized_variable_names = utils_lib.get_assignment_map_from_checkpoint(\n",
        "        variables, init_checkpoint\n",
        "    )\n",
        "\n",
        "    tf.train.init_from_checkpoint(init_checkpoint, assignment_map)\n",
        "\n",
        "    if mode == tf.estimator.ModeKeys.TRAIN:\n",
        "        train_op = optimization.create_optimizer(loss, learning_rate, epoch, num_warmup_steps, False)\n",
        "        estimator_spec = tf.estimator.EstimatorSpec(\n",
        "            mode = mode, loss = loss, train_op = train_op\n",
        "        )\n",
        "\n",
        "    elif mode == tf.estimator.ModeKeys.EVAL:\n",
        "        estimator_spec = tf.estimator.EstimatorSpec(\n",
        "            mode = tf.estimator.ModeKeys.EVAL,\n",
        "            loss = loss,\n",
        "            eval_metric_ops = {'accuracy': accuracy},\n",
        "        )\n",
        "\n",
        "    return estimator_spec"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OotrXz8HCSMg"
      },
      "source": [
        "# Initiate training session\n",
        "train_dataset = get_dataset()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xg2Wmv2JK3MR"
      },
      "source": [
        "# from google.colab import files\n",
        "# src = list(files.upload().values())[0]\n",
        "# open('__init__.py','wb').write(src)\n",
        "# import utils"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J-_lOAgmCZDI",
        "outputId": "cddd05bd-7120-494a-e20c-33fe959f1aa8"
      },
      "source": [
        "# !pip3 install utils\n",
        "# Reminder: Add utils_lib.py before training. ada dalam folder master doc\n",
        "\n",
        "# from utils_lib import utils_lib\n",
        "import utils_lib\n",
        "\n",
        "train_hooks = [\n",
        "    tf.train.LoggingTensorHook(\n",
        "        ['train_accuracy', 'train_loss'], every_n_iter = 1\n",
        "    )\n",
        "]\n",
        "\n",
        "utils_lib.run_training(\n",
        "    train_fn = train_dataset,\n",
        "    model_fn = model_fn,\n",
        "    model_dir = 'finetuned-bert-base',\n",
        "    num_gpus = 1,\n",
        "    log_step = 1,\n",
        "    save_checkpoint_step = epoch,\n",
        "    max_steps = epoch,\n",
        "    train_hooks = train_hooks,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Using config: {'_model_dir': 'finetuned-bert-base', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': 30, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true\n",
            "graph_options {\n",
            "  rewrite_options {\n",
            "    meta_optimizer_iterations: ONE\n",
            "  }\n",
            "}\n",
            ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f5727f24450>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
            "INFO:tensorflow:Skipping training since max_steps has already saved.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mEfmDhUrGFxZ"
      },
      "source": [
        "# TODO: Use different datasets, Calculate Precision, F1-score, etc."
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}